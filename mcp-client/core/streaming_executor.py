#!/usr/bin/env python3
"""
æµå¼LangChainæ‰§è¡Œå™¨
æ”¯æŒSSEå®æ—¶äº‹ä»¶æ¨é€çš„ä»»åŠ¡æ‰§è¡Œå™¨
"""

import asyncio
import json
import logging
from typing import Dict, List, Any, Optional, AsyncGenerator
from datetime import datetime
from uuid import uuid4

from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_mcp_adapters.tools import load_mcp_tools
from langgraph.prebuilt import create_react_agent
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_anthropic import ChatAnthropic
from langchain_core.tools import BaseTool
from langchain_core.callbacks import CallbackManager

from .api_models import TaskResult, TaskRequest
from .client_manager import ClientManager
from .stream_models import *
from .langchain_executor import LangChainMCPExecutor
from .streaming_callbacks import StreamingToolCallbackHandler
from .message_parser import AgentMessageParser
from .streaming_agent import StreamingAgent
from .debug_logger import debug_logger

logger = logging.getLogger(__name__)


# StreamingToolWrapper å·²è¢« StreamingToolCallbackHandler æ›¿ä»£


class StreamingLangChainExecutor(LangChainMCPExecutor):
    """æµå¼LangChainæ‰§è¡Œå™¨"""
    
    def __init__(self, client_manager: ClientManager, anthropic_api_key: Optional[str] = None, debug_enabled: bool = False):
        super().__init__(client_manager, anthropic_api_key)
        self.active_tasks: Dict[str, StreamTaskStatus] = {}
        self.debug_enabled = debug_enabled
        
        # é…ç½®è°ƒè¯•è®°å½•å™¨
        if debug_enabled:
            debug_logger.enable_debug()
        else:
            debug_logger.disable_debug()
    
    async def execute_task_streaming(self, task_request: TaskRequest) -> AsyncGenerator[StreamEvent, None]:
        """
        æµå¼æ‰§è¡Œä»»åŠ¡ï¼Œç”ŸæˆSSEäº‹ä»¶
        
        Args:
            task_request: ä»»åŠ¡è¯·æ±‚å¯¹è±¡
            
        Yields:
            StreamEvent: ä»»åŠ¡æ‰§è¡Œè¿‡ç¨‹ä¸­çš„å„ç§äº‹ä»¶
        """
        
        print(f"ğŸš¨ğŸš¨ğŸš¨ [EXECUTOR] execute_task_streaming è¢«è°ƒç”¨ï¼")
        print(f"ğŸš¨ğŸš¨ğŸš¨ [EXECUTOR] ä»»åŠ¡æè¿°: {task_request.task_description}")
        
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = f"task_{uuid4().hex[:8]}"
        
        # åˆ›å»ºäº‹ä»¶é˜Ÿåˆ—
        event_queue = asyncio.Queue()
        
        # åˆ›å»ºä»»åŠ¡çŠ¶æ€
        task_status = StreamTaskStatus(
            task_id=task_id,
            vm_id=task_request.vm_id,
            session_id=task_request.session_id,
            mcp_server_name=task_request.mcp_server_name,
            task_description=task_request.task_description,
            status="pending",
            start_time=datetime.now()
        )
        
        self.active_tasks[task_id] = task_status
        
        try:
            # å‘é€ä»»åŠ¡å¼€å§‹äº‹ä»¶
            await self._send_task_start_event(event_queue, task_request, task_id)
            
            # å¼‚æ­¥æ‰§è¡Œä»»åŠ¡
            task = asyncio.create_task(
                self._execute_task_with_events(task_request, task_id, event_queue)
            )
            
            # æµå¼ç”Ÿæˆäº‹ä»¶
            async for event in self._stream_events(event_queue, task):
                yield event
                
        except Exception as e:
            logger.error(f"æµå¼ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            
            # å‘é€é”™è¯¯äº‹ä»¶
            error_event = TaskErrorEvent(
                task_id=task_id,
                error_message=str(e),
                error_type=type(e).__name__
            )
            
            yield StreamEvent(type="error", data=error_event.model_dump())
        
        finally:
            # æ¸…ç†ä»»åŠ¡çŠ¶æ€
            if task_id in self.active_tasks:
                del self.active_tasks[task_id]
    
    async def _execute_task_with_events(self, task_request: TaskRequest, task_id: str, event_queue: asyncio.Queue):
        """æ‰§è¡Œä»»åŠ¡å¹¶å‘é€äº‹ä»¶"""
        
        try:
            logger.info(f"å¼€å§‹æ‰§è¡Œæµå¼ä»»åŠ¡ {task_id}")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.active_tasks[task_id].status = "running"
            
            # 1. è·å–æˆ–åˆ›å»ºæµå¼Agent
            logger.info(f"åˆ›å»ºæµå¼Agent for {task_request.vm_id}/{task_request.session_id}")
            print(f"ğŸ¯ğŸ¯ğŸ¯ [DEBUG] ä½¿ç”¨æ–°çš„æµå¼Agentä»£ç ï¼ä»»åŠ¡ID: {task_id}")
            print(f"ğŸ¯ğŸ¯ğŸ¯ [DEBUG] å³å°†è°ƒç”¨ _get_streaming_agent_v2")
            
            streaming_agent = await self._get_streaming_agent_v2(
                task_request.vm_id, 
                task_request.session_id,
                task_request.mcp_server_name
            )
            
            # 2. æ„å»ºä»»åŠ¡æ¶ˆæ¯
            messages = self._build_task_messages(task_request)
            logger.info(f"æ„å»ºäº† {len(messages)} ä¸ªæ¶ˆæ¯")
            
            # è®°å½•AIè¾“å…¥ï¼ˆå¦‚æœå¼€å¯è°ƒè¯•ï¼‰
            if self.debug_enabled:
                tools = streaming_agent.tools_list if hasattr(streaming_agent, 'tools_list') else []
                await debug_logger.log_ai_input(
                    messages=messages,
                    tools=tools,
                    metadata={
                        "task_id": task_id,
                        "vm_id": task_request.vm_id,
                        "session_id": task_request.session_id,
                        "task_description": task_request.task_description
                    }
                )
            
            # 3. æ‰§è¡Œä»»åŠ¡ï¼ˆæµå¼ï¼Œå®æ—¶å‘é€å·¥å…·äº‹ä»¶ï¼‰
            start_time = asyncio.get_event_loop().time()
            logger.info(f"å¼€å§‹æ‰§è¡Œæµå¼Agentä»»åŠ¡")
            
            result = await streaming_agent.astream_invoke(
                messages=messages,
                event_queue=event_queue,
                task_id=task_id,
                max_iterations=10
            )
            end_time = asyncio.get_event_loop().time()
            
            logger.info(f"æµå¼Agentä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {end_time - start_time:.2f}ç§’")
            logger.info(f"æ€»å…±æ‰§è¡Œäº† {result.get('total_steps', 0)} ä¸ªå·¥å…·æ­¥éª¤")
            
            # è®°å½•AIè¾“å‡ºï¼ˆå¦‚æœå¼€å¯è°ƒè¯•ï¼‰
            if self.debug_enabled:
                await debug_logger.log_ai_output(
                    response=result,
                    tool_calls=result.get('tool_calls', []),
                    metadata={
                        "task_id": task_id,
                        "execution_time": end_time - start_time,
                        "total_steps": result.get('total_steps', 0),
                        "success": result.get('success', False)
                    }
                )
            
            # 4. å¤„ç†ç»“æœå¹¶å‘é€å®Œæˆäº‹ä»¶
            task_result = await self._process_streaming_result(
                result, 
                task_request, 
                task_id,
                end_time - start_time
            )
            
            logger.info(f"å¤„ç†ä»»åŠ¡ç»“æœå®Œæˆï¼ŒæˆåŠŸ: {task_result.success}")
            
            # è·å–æ€»tokenä½¿ç”¨é‡
            total_token_usage = result.get('total_token_usage', {})
            if total_token_usage:
                logger.info(f"æ€»tokenä½¿ç”¨é‡: {total_token_usage}")
            
            # å‘é€ä»»åŠ¡å®Œæˆäº‹ä»¶
            await self._send_task_complete_event(event_queue, task_result, task_id, total_token_usage)
            
            logger.info(f"ä»»åŠ¡å®Œæˆäº‹ä»¶å·²å‘é€")
            
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}", exc_info=True)
            
            # å‘é€é”™è¯¯äº‹ä»¶
            error_event = TaskErrorEvent(
                task_id=task_id,
                error_message=str(e),
                error_type=type(e).__name__
            )
            
            await event_queue.put(StreamEvent(type="error", data=error_event.model_dump()))
            logger.info(f"é”™è¯¯äº‹ä»¶å·²å‘é€")
    
    async def _get_streaming_agent(self, vm_id: str, session_id: str, task_id: str, event_queue: asyncio.Queue):
        """è·å–å¸¦æµå¼äº‹ä»¶çš„Agentï¼ˆä½¿ç”¨å›è°ƒæœºåˆ¶ï¼‰"""
        
        # 1. æ„å»ºMCPæœåŠ¡å™¨é…ç½®
        mcp_config = await self._build_mcp_config(vm_id, session_id)
        
        # 2. åˆ›å»ºMCPå®¢æˆ·ç«¯
        mcp_client = MultiServerMCPClient(mcp_config)
        
        # 3. è·å–åŸå§‹å·¥å…·
        tools = await mcp_client.get_tools()
        
        # 4. åˆ›å»ºæµå¼å›è°ƒå¤„ç†å™¨
        callback_handler = StreamingToolCallbackHandler(event_queue, task_id)
        
        # 5. åˆ›å»ºå›è°ƒç®¡ç†å™¨
        callback_manager = CallbackManager([callback_handler])
        
        # 6. åˆ›å»ºå¸¦å›è°ƒçš„æ¨¡å‹
        model = ChatAnthropic(
            model="claude-sonnet-4-20250514",
            api_key=self.anthropic_api_key,
            temperature=0,
            callbacks=[callback_handler]  # æ·»åŠ å›è°ƒå¤„ç†å™¨
        )
        
        # 7. åˆ›å»ºç³»ç»Ÿæç¤º
        system_prompt = self._build_system_prompt(tools)
        
        # 8. åˆ›å»ºAgentï¼Œé…ç½®å›è°ƒ
        agent = create_react_agent(
            model=model,
            tools=tools,
            prompt=system_prompt
        )
        
        # 9. é…ç½®Agentå‚æ•°å’Œå›è°ƒ
        agent = agent.with_config({
            "recursion_limit": 10,
            "max_execution_time": 120,
            "callbacks": [callback_handler],  # Agentçº§åˆ«çš„å›è°ƒ
            "configurable": {
                "thread_id": f"stream_thread_{task_id}",
            }
        })
        
        logger.info(f"ä¸ºä»»åŠ¡ {task_id} åˆ›å»ºäº†æµå¼Agentï¼ŒåŒ…å« {len(tools)} ä¸ªå·¥å…·å’Œå›è°ƒå¤„ç†å™¨")
        
        return agent, callback_handler
    
    async def _get_simple_agent(self, vm_id: str, session_id: str):
        """è·å–ç®€å•çš„Agentï¼ˆä¸ä½¿ç”¨å›è°ƒæœºåˆ¶ï¼‰"""
        
        # 1. æ„å»ºMCPæœåŠ¡å™¨é…ç½®
        mcp_config = await self._build_mcp_config(vm_id, session_id)
        
        # 2. åˆ›å»ºMCPå®¢æˆ·ç«¯
        mcp_client = MultiServerMCPClient(mcp_config)
        
        # 3. è·å–å·¥å…·
        tools = await mcp_client.get_tools()
        
        # 4. åˆ›å»ºæ¨¡å‹
        model = ChatAnthropic(
            model="claude-sonnet-4-20250514",
            api_key=self.anthropic_api_key,
            temperature=0
        )
        
        # 5. åˆ›å»ºç³»ç»Ÿæç¤º
        system_prompt = self._build_system_prompt(tools)
        
        # 6. åˆ›å»ºAgent
        agent = create_react_agent(
            model=model,
            tools=tools,
            prompt=system_prompt
        )
        
        logger.info(f"åˆ›å»ºäº†ç®€å•Agentï¼ŒåŒ…å« {len(tools)} ä¸ªå·¥å…·")
        
        return agent
    
    async def _get_streaming_agent_v2(self, vm_id: str, session_id: str, mcp_server_name: Optional[str] = None) -> StreamingAgent:
        """è·å–æµå¼Agentï¼ˆv2ç‰ˆæœ¬ï¼Œä½¿ç”¨è‡ªå®šä¹‰æµå¼æ‰§è¡Œï¼‰"""
        
        # 1. æ„å»ºMCPæœåŠ¡å™¨é…ç½®ï¼ˆå¯é€‰è¿‡æ»¤ç‰¹å®šæœåŠ¡å™¨ï¼‰
        mcp_config = await self._build_mcp_config(vm_id, session_id, mcp_server_name)
        
        if mcp_server_name:
            print(f"ğŸ¯ [DEBUG] è¿‡æ»¤åˆ°æŒ‡å®šMCPæœåŠ¡å™¨: {mcp_server_name}")
            print(f"ğŸ¯ [DEBUG] MCPé…ç½®åŒ…å«æœåŠ¡å™¨: {list(mcp_config.keys())}")
        else:
            print(f"ğŸ¯ [DEBUG] ä½¿ç”¨æ‰€æœ‰å¯ç”¨MCPæœåŠ¡å™¨: {list(mcp_config.keys())}")
        
        # 2. åˆ›å»ºMCPå®¢æˆ·ç«¯
        mcp_client = MultiServerMCPClient(mcp_config)
        
        # 3. è·å–å·¥å…·å¹¶åŒ…è£…ä»¥å¤„ç†å‚æ•°æ ¼å¼
        raw_tools = await mcp_client.get_tools()
        
        # Debug: æ£€æŸ¥åŸå§‹å·¥å…·ä¿¡æ¯
        print(f"ğŸ” [DEBUG] åŸå§‹å·¥å…·æ•°é‡: {len(raw_tools)}")
        for i, tool in enumerate(raw_tools[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"ğŸ” [DEBUG] åŸå§‹å·¥å…· {i}: name={getattr(tool, 'name', 'NO_NAME')}, desc={getattr(tool, 'description', 'NO_DESC')[:50]}")
        
        tools = self._wrap_mcp_tools_for_langchain(raw_tools)
        
        # Debug: æ£€æŸ¥åŒ…è£…åå·¥å…·ä¿¡æ¯
        print(f"ğŸ” [DEBUG] åŒ…è£…åå·¥å…·æ•°é‡: {len(tools)}")
        for i, tool in enumerate(tools[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"ğŸ” [DEBUG] åŒ…è£…åå·¥å…· {i}: name={getattr(tool, 'name', 'NO_NAME')}, desc={getattr(tool, 'description', 'NO_DESC')[:50]}")
        
        # 4. åˆ›å»ºæ¨¡å‹
        model = ChatAnthropic(
            model="claude-sonnet-4-20250514",
            api_key=self.anthropic_api_key,
            temperature=0
        )
        
        # 5. åˆ›å»ºç³»ç»Ÿæç¤º
        system_prompt_obj = self._build_system_prompt(tools)
        system_prompt_text = system_prompt_obj.content
        
        # 6. åˆ›å»ºæµå¼Agent
        streaming_agent = StreamingAgent(
            model=model,
            tools=tools,
            system_prompt=system_prompt_text
        )
        
        logger.info(f"åˆ›å»ºäº†æµå¼Agentï¼ŒåŒ…å« {len(tools)} ä¸ªå·¥å…·")
        
        return streaming_agent
    
    async def _stream_events(self, event_queue: asyncio.Queue, task: asyncio.Task) -> AsyncGenerator[StreamEvent, None]:
        """æµå¼ç”Ÿæˆäº‹ä»¶"""
        
        while not task.done():
            try:
                # ç­‰å¾…äº‹ä»¶ï¼Œè¶…æ—¶æ—¶é—´çŸ­ä»¥ä¿æŒå“åº”æ€§
                event = await asyncio.wait_for(event_queue.get(), timeout=0.1)
                yield event
            except asyncio.TimeoutError:
                # è¶…æ—¶ç»§ç»­å¾ªç¯
                continue
        
        # ç­‰å¾…ä»»åŠ¡å®Œæˆ
        try:
            await task
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        
        # ä»»åŠ¡å®Œæˆåï¼Œå¤„ç†é˜Ÿåˆ—ä¸­å‰©ä½™çš„äº‹ä»¶
        remaining_events = 0
        while not event_queue.empty():
            try:
                event = event_queue.get_nowait()
                yield event
                remaining_events += 1
            except asyncio.QueueEmpty:
                break
        
        # çŸ­æš‚ç­‰å¾…ç¡®ä¿æ‰€æœ‰äº‹ä»¶éƒ½è¢«å¤„ç†
        await asyncio.sleep(0.1)
        
        # å†æ¬¡æ£€æŸ¥æ˜¯å¦æœ‰æ–°äº‹ä»¶
        while not event_queue.empty():
            try:
                event = event_queue.get_nowait()
                yield event
                remaining_events += 1
            except asyncio.QueueEmpty:
                break
        
        logger.info(f"æµå¼äº‹ä»¶å¤„ç†å®Œæˆï¼Œå¤„ç†äº† {remaining_events} ä¸ªå‰©ä½™äº‹ä»¶")
    
    async def _send_task_start_event(self, event_queue: asyncio.Queue, task_request: TaskRequest, task_id: str):
        """å‘é€ä»»åŠ¡å¼€å§‹äº‹ä»¶"""
        event_data = TaskStartEvent(
            task_id=task_id,
            vm_id=task_request.vm_id,
            session_id=task_request.session_id,
            mcp_server_name=task_request.mcp_server_name,
            task_description=task_request.task_description,
            context=getattr(task_request, 'context', None)
        )
        
        stream_event = StreamEvent(
            type="start",
            data=event_data.model_dump()
        )
        
        await event_queue.put(stream_event)
    
    async def _send_task_complete_event(self, event_queue: asyncio.Queue, task_result: TaskResult, task_id: str, total_token_usage: Optional[Dict[str, int]] = None):
        """å‘é€ä»»åŠ¡å®Œæˆäº‹ä»¶"""
        event_data = TaskCompleteEvent(
            task_id=task_id,
            success=task_result.success,
            final_result=task_result.final_result,
            summary=task_result.summary,
            execution_time=task_result.execution_time_seconds,
            total_steps=len(task_result.execution_steps),
            successful_steps=sum(1 for step in task_result.execution_steps if step.get("status") == "success"),
            new_files=task_result.new_files,
            total_token_usage=total_token_usage
        )
        
        stream_event = StreamEvent(
            type="complete",
            data=event_data.model_dump()
        )
        
        await event_queue.put(stream_event)
    
    async def _process_streaming_result(self, result: Dict, task_request: TaskRequest, 
                                      task_id: str, execution_time: float) -> TaskResult:
        """å¤„ç†æµå¼ç»“æœï¼Œå¤ç”¨çˆ¶ç±»é€»è¾‘"""
        
        # å¤ç”¨çˆ¶ç±»çš„ç»“æœå¤„ç†é€»è¾‘
        task_result = await self._process_result(result, task_request, execution_time)
        
        # æ›´æ–°ä»»åŠ¡ID
        task_result.task_id = task_id
        
        return task_result
    
    def _extract_final_result(self, message) -> str:
        """ä»æ¶ˆæ¯ä¸­æå–æœ€ç»ˆç»“æœï¼Œå¤„ç†ä¸åŒçš„å†…å®¹æ ¼å¼"""
        
        if not message or not hasattr(message, 'content'):
            return "ä»»åŠ¡å®Œæˆ"
        
        content = message.content
        
        # å¦‚æœcontentæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
        if isinstance(content, str):
            return content
        
        # å¦‚æœcontentæ˜¯åˆ—è¡¨ï¼Œæå–æ–‡æœ¬å†…å®¹
        if isinstance(content, list):
            text_parts = []
            for item in content:
                if isinstance(item, dict):
                    # å¤„ç† {'text': '...', 'type': '...'} æ ¼å¼
                    if 'text' in item:
                        text_parts.append(item['text'])
                    elif 'content' in item:
                        text_parts.append(str(item['content']))
                elif isinstance(item, str):
                    text_parts.append(item)
                else:
                    text_parts.append(str(item))
            
            if text_parts:
                return ' '.join(text_parts)
        
        # å¦‚æœcontentæ˜¯å­—å…¸ï¼Œå°è¯•æå–æ–‡æœ¬
        if isinstance(content, dict):
            if 'text' in content:
                return content['text']
            elif 'content' in content:
                return str(content['content'])
        
        # æœ€åå…œåº•ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        try:
            return str(content)
        except Exception:
            return "ä»»åŠ¡å®Œæˆ"
    
    def get_active_tasks(self) -> Dict[str, StreamTaskStatus]:
        """è·å–æ´»è·ƒä»»åŠ¡çŠ¶æ€"""
        return self.active_tasks.copy()
    
    def _wrap_mcp_tools_for_langchain(self, raw_tools: List[BaseTool]) -> List[BaseTool]:
        """åŒ…è£…MCPå·¥å…·ä»¥å¤„ç†LangChainå’ŒMCPä¹‹é—´çš„å‚æ•°æ ¼å¼å·®å¼‚"""
        from langchain_core.tools import StructuredTool
        from typing import Type
        from pydantic import BaseModel, Field
        
        wrapped_tools = []
        
        for tool in raw_tools:
            # å¦‚æœå·¥å…·å·²ç»æ˜¯æ­£ç¡®æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
            if not self._tool_needs_wrapping(tool):
                wrapped_tools.append(tool)
                continue
            
            # ä¸ºéœ€è¦å‚æ•°åŒ…è£…çš„å·¥å…·åˆ›å»ºåŒ…è£…ç‰ˆæœ¬
            try:
                # è·å–åŸå§‹å·¥å…·çš„schema
                original_schema = tool.args_schema
                if not original_schema:
                    wrapped_tools.append(tool)
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åµŒå¥—çš„"req"ç»“æ„
                if hasattr(original_schema, 'model_fields') and 'req' in original_schema.model_fields:
                    # åˆ›å»ºæ‰å¹³åŒ–çš„å‚æ•°æ¨¡å‹
                    req_field = original_schema.model_fields['req']
                    if hasattr(req_field, 'annotation') and hasattr(req_field.annotation, 'model_fields'):
                        inner_fields = req_field.annotation.model_fields
                        
                        # åˆ›å»ºæ–°çš„æ‰å¹³åŒ–æ¨¡å‹
                        flat_fields = {}
                        for field_name, field_info in inner_fields.items():
                            flat_fields[field_name] = (field_info.annotation, field_info)
                        
                        # åŠ¨æ€åˆ›å»ºPydanticæ¨¡å‹
                        FlatArgsModel = type(
                            f"{tool.name}FlatArgs",
                            (BaseModel,),
                            {'__annotations__': {k: v[0] for k, v in flat_fields.items()}}
                        )
                        
                        # ä¸ºæ¯ä¸ªå­—æ®µè®¾ç½®é»˜è®¤å€¼
                        for field_name, (field_type, field_info) in flat_fields.items():
                            if hasattr(field_info, 'default'):
                                setattr(FlatArgsModel, field_name, field_info.default)
                        
                        # åˆ›å»ºåŒ…è£…å‡½æ•°
                        async def wrapped_func(**kwargs):
                            # ç›´æ¥ä¼ é€’å¹³çº§å‚æ•°ï¼ˆMCP Server ç«¯ç°åœ¨æœŸæœ›å¹³çº§å‚æ•°ï¼‰
                            if hasattr(tool, 'ainvoke'):
                                return await tool.ainvoke(kwargs)
                            else:
                                return tool.invoke(kwargs)
                        
                        # åˆ›å»ºæ–°çš„StructuredTool
                        wrapped_tool = StructuredTool(
                            name=tool.name,
                            description=tool.description,
                            func=wrapped_func,
                            args_schema=FlatArgsModel,
                            coroutine=True
                        )
                        
                        print(f"ğŸ”§ [WRAPPER] åŒ…è£…äº†å·¥å…·: {tool.name}")
                        wrapped_tools.append(wrapped_tool)
                    else:
                        wrapped_tools.append(tool)
                else:
                    wrapped_tools.append(tool)
                    
            except Exception as e:
                print(f"âŒ [WRAPPER] åŒ…è£…å·¥å…· {tool.name} æ—¶å‡ºé”™: {e}")
                # å‡ºé”™æ—¶ä½¿ç”¨åŸå§‹å·¥å…·
                wrapped_tools.append(tool)
        
        print(f"ğŸ”§ [WRAPPER] æ€»å…±åŒ…è£…äº† {len(wrapped_tools)} ä¸ªå·¥å…·")
        return wrapped_tools
    
    def _tool_needs_wrapping(self, tool: BaseTool) -> bool:
        """æ£€æŸ¥å·¥å…·æ˜¯å¦éœ€è¦å‚æ•°åŒ…è£…"""
        # ç°åœ¨ MCP Server ç«¯å·¥å…·ä½¿ç”¨å¹³çº§å‚æ•°ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ä¸éœ€è¦åŒ…è£…
        # åªæœ‰å½“å·¥å…·schemaæ˜ç¡®åŒ…å«åµŒå¥—reqç»“æ„æ—¶æ‰éœ€è¦åŒ…è£…
        if not hasattr(tool, 'args_schema') or not tool.args_schema:
            return False
        
        # æ£€æŸ¥æ˜¯å¦æœ‰åµŒå¥—çš„reqç»“æ„
        if hasattr(tool.args_schema, 'model_fields') and 'req' in tool.args_schema.model_fields:
            return True
        
        return False